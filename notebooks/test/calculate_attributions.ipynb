{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from captum.attr import DeepLift, DeepLiftShap, GradientShap, InputXGradient, IntegratedGradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "dataset = h5py.File(\"/cellar/users/aklie/projects/ML4GLand/use_cases/deAlmeida22/data/evo_aug/DeepSTARR_data.h5\", 'r')\n",
    "x_test = np.array(dataset['X_test']).astype(np.float32)\n",
    "y_test = np.array(dataset['Y_test']).astype(np.float32)\n",
    "dataset.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eugene import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.DeepSTARR.load_from_checkpoint(\"/cellar/users/aklie/projects/ML4GLand/use_cases/deAlmeida22/models/eugene/DeepSTARR.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr = DeepLiftShap(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_np = x_test[:10]\n",
    "test_torch = torch.from_numpy(test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seqpro as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 4, 249])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselines = torch.from_numpy(sp.dinuc_shuffle_seqs(test_np, num_shufs=10)) \n",
    "#baselines = torch.from_numpy(sp.dinuc_shuffle_seqs(test_np))\n",
    "baselines.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flip 1st and 2nd dimension\n",
    "baselines = baselines.permute(1, 0, 2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 4, 249])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baselines.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Baseline can be provided as a tensor for just one input and broadcasted to the batch or input and baseline must have the same shape or the baseline corresponding to each input tensor must be a scalar. Found baseline: tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 1.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 1., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 1., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 1.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 1.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.]],\n\n         [[1., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 1.,  ..., 1., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        ...,\n\n\n        [[[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 1., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 1., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 1., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.]],\n\n         [[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 1.,  ..., 1., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 1.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 1., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]], requires_grad=True) and input: tensor([[[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        ...,\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1699884/3060574303.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_torch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mbaselines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_core/deep_lift.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, custom_attribution_func)\u001b[0m\n\u001b[1;32m    864\u001b[0m                 \u001b[0mLiteral\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             ),\n\u001b[0;32m--> 866\u001b[0;31m             \u001b[0mcustom_attribution_func\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_attribution_func\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    867\u001b[0m         )\n\u001b[1;32m    868\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_core/deep_lift.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, custom_attribution_func)\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0mgradient_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapply_gradient_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 333\u001b[0;31m         \u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \u001b[0;31m# set hooks for baselines\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_utils/common.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(inputs, baselines, n_steps, method, draw_baseline_from_distrib)\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdraw_baseline_from_distrib\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m ) -> None:\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0m_validate_input_basic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_baseline_from_distrib\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     assert (\n\u001b[1;32m     47\u001b[0m         \u001b[0mn_steps\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(inputs, baselines, draw_baseline_from_distrib)\u001b[0m\n\u001b[1;32m    104\u001b[0m                 \u001b[0;34m\" same shape or the baseline corresponding to each input tensor\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \" must be a scalar. Found baseline: {} and input: {}\".format(\n\u001b[0;32m--> 106\u001b[0;31m                     \u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m                 )\n\u001b[1;32m    108\u001b[0m             )\n",
      "\u001b[0;31mAssertionError\u001b[0m: Baseline can be provided as a tensor for just one input and broadcasted to the batch or input and baseline must have the same shape or the baseline corresponding to each input tensor must be a scalar. Found baseline: tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 1.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 1., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 1., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 1.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 1.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.]],\n\n         [[1., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 1.,  ..., 1., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        ...,\n\n\n        [[[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 1., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 1., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 1., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.]],\n\n         [[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 1.,  ..., 1., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]],\n\n\n        [[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 1.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 1., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 0., 1., 0.]],\n\n         [[1., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]]]], requires_grad=True) and input: tensor([[[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        ...,\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]],\n\n        [[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]], requires_grad=True)"
     ]
    }
   ],
   "source": [
    "attr.attribute(\n",
    "    inputs=test_torch,\n",
    "    baselines=baselines,\n",
    "    target=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "The samples in input and baseline batches must have the same shape or the baseline corresponding to the input tensor must be a scalar. Found baseline: tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.]]],\n\n\n        [[[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 0., 1., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 1.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 1.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 1., 0.]]],\n\n\n        ...,\n\n\n        [[[1., 0., 1.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.]],\n\n         [[1., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.]],\n\n         [[1., 0., 1.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.]],\n\n         ...,\n\n         [[1., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.]],\n\n         [[1., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.]],\n\n         [[1., 0., 1.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 1.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]]],\n\n\n        [[[0., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         ...,\n\n         [[0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.],\n          [1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.],\n          [1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]]]]) and input: tensor([[[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        [[1., 0., 0.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 1., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 1.],\n         [0., 1., 0.,  ..., 0., 0., 0.],\n         [1., 0., 1.,  ..., 1., 1., 0.]],\n\n        ...,\n\n        [[1., 0., 0.,  ..., 0., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 1., 1.,  ..., 1., 0., 1.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [1., 1., 0.,  ..., 1., 0., 1.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 0.]],\n\n        [[0., 1., 0.,  ..., 0., 1., 0.],\n         [0., 0., 0.,  ..., 1., 0., 1.],\n         [1., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1699884/3263539362.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mn_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mstdevs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m )\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_core/gradient_shap.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, n_samples, stdevs, target, additional_forward_args, return_convergence_delta)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         )\n\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_core/noise_tunnel.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, nt_type, nt_samples, nt_samples_batch_size, stdevs, draw_baseline_from_distrib, **kwargs)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m             \u001b[0mkwargs_copy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m             \u001b[0mexpand_partial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt_samples_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_copy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mattr_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribution_method\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_core/noise_tunnel.py\u001b[0m in \u001b[0;36mexpand_partial\u001b[0;34m(nt_samples_partition, kwargs_partial)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mnt_samples_partition\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0mkwargs_partial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m                 \u001b[0mdraw_baseline_from_distrib\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdraw_baseline_from_distrib\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m             )\n\u001b[1;32m    288\u001b[0m             \u001b[0m_expand_and_update_feature_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnt_samples_partition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs_partial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_expand_and_update_baselines\u001b[0;34m(inputs, n_samples, kwargs, draw_baseline_from_distrib)\u001b[0m\n\u001b[1;32m    301\u001b[0m     \u001b[0mbaselines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_format_baseline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    302\u001b[0m     _validate_input(\n\u001b[0;32m--> 303\u001b[0;31m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaselines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdraw_baseline_from_distrib\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdraw_baseline_from_distrib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    304\u001b[0m     )\n\u001b[1;32m    305\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(inputs, baselines, draw_baseline_from_distrib)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0;34m\" the same shape or the baseline corresponding to the\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;34m\" input tensor must be a scalar.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0;34m\" Found baseline: {} and input: {} \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbaseline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m             )\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: The samples in input and baseline batches must have the same shape or the baseline corresponding to the input tensor must be a scalar. Found baseline: tensor([[[[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 1., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 1.],\n          [0., 0., 0.,  ..., 1., 0., 0.]]],\n\n\n        [[[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 1.,  ..., 0., 1., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         ...,\n\n         [[1., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 1.,  ..., 1., 0., 0.]],\n\n         [[1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[1., 1., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 1.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 1., 0.]]],\n\n\n        ...,\n\n\n        [[[1., 0., 1.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.]],\n\n         [[1., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 1.]],\n\n         [[1., 0., 1.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.]],\n\n         ...,\n\n         [[1., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.]],\n\n         [[1., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.]],\n\n         [[1., 0., 1.,  ..., 1., 1., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.]]],\n\n\n        [[[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 1., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 1.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]],\n\n         ...,\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 1.,  ..., 0., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 1., 1., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 0., 0.],\n          [1., 0., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 1.,  ..., 0., 1., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 0., 0.],\n          [1., 1., 0.,  ..., 1., 0., 1.],\n          [0., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 1., 0.]]],\n\n\n        [[[0., 1., 0.,  ..., 1., 1., 0.],\n          [0., 0., 1.,  ..., 0., 0., 1.],\n          [1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 1., 0.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 1.,  ..., 0., 1., 0.],\n          [0., 0., 0.,  ..., 1., 0., 1.],\n          [1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]],\n\n         ...,\n\n         [[0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 1., 0.,  ..., 0., 0., 1.],\n          [1., 0., 1.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 1., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.],\n          [1., 1., 0.,  ..., 0., 0., 0.],\n          [0., 0., 0.,  ..., 0., 0., 0.]],\n\n         [[0., 0., 0.,  ..., 0., 1., 0.],\n          [0., 0., 1.,  ..., 1., 0., 1.],\n          [1., 0., 0.,  ..., 0., 0., 0.],\n          [0., 1., 0.,  ..., 0., 0., 0.]]]]) and input: tensor([[[1., 1., 0.,  ..., 0., 0., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 1.],\n         [0., 0., 0.,  ..., 1., 0., 0.]],\n\n        [[1., 0., 0.,  ..., 0., 0., 1.],\n         [0., 0., 0.,  ..., 1., 1., 0.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 1., 0.,  ..., 0., 0., 0.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 1.],\n         [0., 1., 0.,  ..., 0., 0., 0.],\n         [1., 0., 1.,  ..., 1., 1., 0.]],\n\n        ...,\n\n        [[1., 0., 0.,  ..., 0., 1., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.],\n         [0., 1., 1.,  ..., 1., 0., 1.]],\n\n        [[0., 0., 0.,  ..., 0., 0., 0.],\n         [1., 1., 0.,  ..., 1., 0., 1.],\n         [0., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 1., 0.]],\n\n        [[0., 1., 0.,  ..., 0., 1., 0.],\n         [0., 0., 0.,  ..., 1., 0., 1.],\n         [1., 0., 1.,  ..., 0., 0., 0.],\n         [0., 0., 0.,  ..., 0., 0., 0.]]]) "
     ]
    }
   ],
   "source": [
    "attr.attribute(\n",
    "    inputs=test_torch,\n",
    "    baselines=baselines,\n",
    "    target=0,\n",
    "    n_samples=10,\n",
    "    stdevs=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m\n",
      "\u001b[0mattr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattribute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mTensorOrTupleOfTensorsGeneric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mbaselines\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensorOrTupleOfTensorsGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m~\u001b[0m\u001b[0mTensorOrTupleOfTensorsGeneric\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mn_samples\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstdevs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_convergence_delta\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensorOrTupleOfTensorsGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mTensorOrTupleOfTensorsGeneric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Args:\n",
      "\n",
      "    inputs (tensor or tuple of tensors):  Input for which SHAP attribution\n",
      "                values are computed. If `forward_func` takes a single\n",
      "                tensor as input, a single input tensor should be provided.\n",
      "                If `forward_func` takes multiple tensors as input, a tuple\n",
      "                of the input tensors should be provided. It is assumed\n",
      "                that for all given input tensors, dimension 0 corresponds\n",
      "                to the number of examples, and if multiple input tensors\n",
      "                are provided, the examples must be aligned appropriately.\n",
      "    baselines (tensor, tuple of tensors, callable):\n",
      "                Baselines define the starting point from which expectation\n",
      "                is computed and can be provided as:\n",
      "\n",
      "                - a single tensor, if inputs is a single tensor, with\n",
      "                  the first dimension equal to the number of examples\n",
      "                  in the baselines' distribution. The remaining dimensions\n",
      "                  must match with input tensor's dimension starting from\n",
      "                  the second dimension.\n",
      "\n",
      "                - a tuple of tensors, if inputs is a tuple of tensors,\n",
      "                  with the first dimension of any tensor inside the tuple\n",
      "                  equal to the number of examples in the baseline's\n",
      "                  distribution. The remaining dimensions must match\n",
      "                  the dimensions of the corresponding input tensor\n",
      "                  starting from the second dimension.\n",
      "\n",
      "                - callable function, optionally takes `inputs` as an\n",
      "                  argument and either returns a single tensor\n",
      "                  or a tuple of those.\n",
      "\n",
      "                It is recommended that the number of samples in the baselines'\n",
      "                tensors is larger than one.\n",
      "    n_samples (int, optional):  The number of randomly generated examples\n",
      "                per sample in the input batch. Random examples are\n",
      "                generated by adding gaussian random noise to each sample.\n",
      "                Default: `5` if `n_samples` is not provided.\n",
      "    stdevs    (float, or a tuple of floats optional): The standard deviation\n",
      "                of gaussian noise with zero mean that is added to each\n",
      "                input in the batch. If `stdevs` is a single float value\n",
      "                then that same value is used for all inputs. If it is\n",
      "                a tuple, then it must have the same length as the inputs\n",
      "                tuple. In this case, each stdev value in the stdevs tuple\n",
      "                corresponds to the input with the same index in the inputs\n",
      "                tuple.\n",
      "                Default: 0.0\n",
      "    target (int, tuple, tensor or list, optional):  Output indices for\n",
      "                which gradients are computed (for classification cases,\n",
      "                this is usually the target class).\n",
      "                If the network returns a scalar value per example,\n",
      "                no target index is necessary.\n",
      "                For general 2D outputs, targets can be either:\n",
      "\n",
      "                - a single integer or a tensor containing a single\n",
      "                  integer, which is applied to all input examples\n",
      "\n",
      "                - a list of integers or a 1D tensor, with length matching\n",
      "                  the number of examples in inputs (dim 0). Each integer\n",
      "                  is applied as the target for the corresponding example.\n",
      "\n",
      "                For outputs with > 2 dimensions, targets can be either:\n",
      "\n",
      "                - A single tuple, which contains #output_dims - 1\n",
      "                  elements. This target index is applied to all examples.\n",
      "\n",
      "                - A list of tuples with length equal to the number of\n",
      "                  examples in inputs (dim 0), and each tuple containing\n",
      "                  #output_dims - 1 elements. Each tuple is applied as the\n",
      "                  target for the corresponding example.\n",
      "\n",
      "                Default: None\n",
      "    additional_forward_args (any, optional): If the forward function\n",
      "                requires additional arguments other than the inputs for\n",
      "                which attributions should not be computed, this argument\n",
      "                can be provided. It can contain a tuple of ND tensors or\n",
      "                any arbitrary python type of any shape.\n",
      "                In case of the ND tensor the first dimension of the\n",
      "                tensor must correspond to the batch size. It will be\n",
      "                repeated for each `n_steps` for each randomly generated\n",
      "                input sample.\n",
      "                Note that the gradients are not computed with respect\n",
      "                to these arguments.\n",
      "                Default: None\n",
      "    return_convergence_delta (bool, optional): Indicates whether to return\n",
      "                convergence delta or not. If `return_convergence_delta`\n",
      "                is set to True convergence delta will be returned in\n",
      "                a tuple following attributions.\n",
      "                Default: False\n",
      "Returns:\n",
      "    **attributions** or 2-element tuple of **attributions**, **delta**:\n",
      "    - **attributions** (*tensor* or tuple of *tensors*):\n",
      "                Attribution score computed based on GradientSHAP with respect\n",
      "                to each input feature. Attributions will always be\n",
      "                the same size as the provided inputs, with each value\n",
      "                providing the attribution of the corresponding input index.\n",
      "                If a single tensor is provided as inputs, a single tensor is\n",
      "                returned. If a tuple is provided for inputs, a tuple of\n",
      "                corresponding sized tensors is returned.\n",
      "    - **delta** (*tensor*, returned if return_convergence_delta=True):\n",
      "                This is computed using the property that the total\n",
      "                sum of forward_func(inputs) - forward_func(baselines)\n",
      "                must be very close to the total sum of the attributions\n",
      "                based on GradientSHAP.\n",
      "                Delta is calculated for each example in the input after adding\n",
      "                `n_samples` times gaussian noise to each of them. Therefore,\n",
      "                the dimensionality of the deltas tensor is equal to the\n",
      "                `number of examples in the input` * `n_samples`\n",
      "                The deltas are ordered by each input example and `n_samples`\n",
      "                noisy samples generated for it.\n",
      "\n",
      "Examples::\n",
      "\n",
      "    >>> # ImageClassifier takes a single input tensor of images Nx3x32x32,\n",
      "    >>> # and returns an Nx10 tensor of class probabilities.\n",
      "    >>> net = ImageClassifier()\n",
      "    >>> gradient_shap = GradientShap(net)\n",
      "    >>> input = torch.randn(3, 3, 32, 32, requires_grad=True)\n",
      "    >>> # choosing baselines randomly\n",
      "    >>> baselines = torch.randn(20, 3, 32, 32)\n",
      "    >>> # Computes gradient shap for the input\n",
      "    >>> # Attribution size matches input size: 3x3x32x32\n",
      "    >>> attribution = gradient_shap.attribute(input, baselines,\n",
      "                                                        target=5)\n",
      "\u001b[0;31mFile:\u001b[0m      ~/opt/miniconda3/envs/eugene_dev/lib/python3.7/site-packages/captum/attr/_core/gradient_shap.py\n",
      "\u001b[0;31mType:\u001b[0m      method\n"
     ]
    }
   ],
   "source": [
    "attr.attribute(\n",
    "    inputs=test_torch,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 eugene_dev",
   "language": "python",
   "name": "eugene_dev"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
